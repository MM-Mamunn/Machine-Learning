# -*- coding: utf-8 -*-
"""technical-v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eyj_GdGbXX_MUilv7G04Fvxd37J6gLjT
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report


from sklearn.preprocessing  import StandardScaler
from imblearn.over_sampling import RandomOverSampler

# Deep Learning imports
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
import tensorflow as  tf

# Load dataset
data_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian%20Liver%20Patient%20Dataset%20(ILPD).csv"
df = pd.read_csv(data_url, header=None)
df.columns = ["Age", "Gender", "TB", "DB", "Alkphos", "Sgpt", "Sgot", "TP", "ALB", "A/G Ratio", "Liver Disease"]
df.dropna(inplace=True)
# Encode categorical variable
# df["Gender"] = LabelEncoder().fit_transform(df["Gender"])
df["Gender"] = (df["Gender"] == 'Male').astype(int)
df["Liver Disease"] = (df["Liver Disease"] == 1).astype(int)
print(df)

for label in df.columns[:-1]:
  plt.hist(df[df["Gender"] == 1][label], color ='blue' , label = 'gamma', alpha= 0.7 ,density = True )
  plt.hist(df[df["Gender"] == 0][label], color ='red' , label = 'hadron', alpha= 0.7 ,density = True )
  plt.title(label)
  plt.ylabel("Probability")
  plt.xlabel(label)
  plt.legend()
  plt.show()

# Balance dataset using SMOTE
from imblearn.over_sampling import SMOTE
X = df.drop(columns=["Liver Disease"])
y = df["Liver Disease"]
smote = SMOTE(random_state=42)
X, y = smote.fit_resample(X, y)
# convert labels from {1,2} → {0,1}
y = y - 1

# Split dataset
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
train, valid , test = np.split(df.sample(frac = 1), [int(0.75 * len(df)) , int(0.8 * len(df))])

# Standardize features
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

def scale_dataset(dataframe, oversample = False):
  x = dataframe[dataframe.columns[:-1]].values
  y = dataframe[dataframe.columns[-1]].values

  scaler = StandardScaler()
  x = scaler.fit_transform(x)
  if oversample:
    ros = RandomOverSampler()
    x,y =ros.fit_resample(x,y)
  data = np.hstack((x,np.reshape(y,(-1,1))))
  return data,x,y

# ----- Machine Learning Models -----
# include additional ensemble and tuned models
# from sklearn.ensemble import BaggingClassifier, VotingClassifier, StackingClassifier

train, X_train, y_train = scale_dataset(train, oversample = True)
valid, X_valid, y_valid = scale_dataset(valid, oversample = True)
test, X_test, y_test = scale_dataset(test, oversample = False)

# define base estimators
base_knn = KNeighborsClassifier()
base_svc = SVC(probability=True)
base_rf = RandomForestClassifier(random_state=42)
base_gb = GradientBoostingClassifier(random_state=42)

# tuned versions via GridSearchCV
tuned_knn = GridSearchCV(KNeighborsClassifier(), {'n_neighbors':[5,7,9], 'weights':['uniform','distance']}, cv=3)

tuned_svc = GridSearchCV(SVC(probability=True), {'C':[1,10], 'gamma':[0.01,0.1]}, cv=3)

tuned_rf = GridSearchCV(RandomForestClassifier(random_state=42), {'n_estimators':[100,200], 'max_depth':[5,10]}, cv=3)

tuned_gb = GridSearchCV(GradientBoostingClassifier(random_state=42), {'n_estimators':[100,200], 'learning_rate':[0.05,0.1]}, cv=3)

tuned_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
                         {'n_estimators':[100,200], 'max_depth':[3,5], 'learning_rate':[0.05,0.1]}, cv=3)

ml_models = {
    "Naive Bayes": GaussianNB(),
    "KNN": KNeighborsClassifier(n_neighbors=7, weights='distance'),
    "Tuned - KNN": tuned_knn,
    "SVM": SVC(kernel='rbf', C=10, gamma=0.1, probability=True),
    "Tuned - SVM": tuned_svc,
    "Logistic Regression": LogisticRegression(solver='liblinear', C=1.0),
    "Decision Tree": DecisionTreeClassifier(max_depth=5, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),
    "Tuned - Random Forest": tuned_rf,
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42),
    "Tuned - Gradient Boosting": tuned_gb,
    "AdaBoost": AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42),
    "XGBoost": XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, use_label_encoder=False, eval_metric='logloss', random_state=42),
    "Tuned - XGBoost": tuned_xgb,
    # ensemble wrappers
    "Ensemble - Bagging": BaggingClassifier(random_state=42),
    "Ensemble - AdaBoost": AdaBoostClassifier(random_state=42),
    "Ensemble - Voting (Hard)": VotingClassifier(estimators=[('rf', base_rf), ('svc', base_svc), ('knn', base_knn)], voting='hard'),
    "Ensemble - Voting (Soft)": VotingClassifier(estimators=[('rf', base_rf), ('svc', base_svc), ('knn', base_knn)], voting='soft'),
    "Ensemble - Stacking": StackingClassifier(estimators=[('rf', base_rf), ('svc', base_svc), ('knn', base_knn)], final_estimator=LogisticRegression())
}

# ----- Deep Learning Models -----
# Prepare labels
y_train_dl = to_categorical(y_train)
y_test_dl = to_categorical(y_test)
input_dim = X_train.shape[1]

dl_models = {}

# 1. Simple MLP
def create_mlp():
    model = Sequential([
        Dense(64, activation='relu', input_shape=(input_dim,)),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(2, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

dl_models['MLP'] = create_mlp()

# 2. Deeper MLP
def create_deep_mlp():
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_dim,)),
        Dropout(0.4),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(2, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

dl_models['Deep MLP'] = create_deep_mlp()

# 3. Wide & Deep
def create_wide_deep():
    model = Sequential([
        Dense(256, activation='relu', input_shape=(input_dim,)),
        Dense(32, activation='relu'),
        Dense(2, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

dl_models['Wide & Deep'] = create_wide_deep()

# 4. Shallow MLP
def create_shallow():
    model = Sequential([
        Dense(32, activation='relu', input_shape=(input_dim,)),
        Dense(2, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

dl_models['Shallow MLP'] = create_shallow()

# 5. MLP with BatchNorm
def create_batchnorm():
    from tensorflow.keras.layers import BatchNormalization
    model = Sequential([
        Dense(64, input_shape=(input_dim,)),
        BatchNormalization(),
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dense(2, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

dl_models['MLP + BatchNorm'] = create_batchnorm()

# --- Train ML models (re‑define or assume ml_models exists) ---
ml_results = {}
for name, model in ml_models.items():
    print(f"Training ML: {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    ml_results[name] = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {ml_results[name]:.4f}\n")

# --- Train DL models (re‑define or assume dl_models exists) ---
dl_results = {}
for name, model in dl_models.items():
    print(f"Training DL: {name}...")
    model.fit(X_train, y_train_dl, epochs=30, batch_size=32,
              validation_split=0.1, verbose=0)
    loss, acc = model.evaluate(X_test, y_test_dl, verbose=0)
    dl_results[name] = acc
    print(f"{name} Test Accuracy: {acc:.4f}\n")

# --- Plot results ---
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
all_results = {**ml_results, **dl_results}       # now both dicts exist
sns.barplot(x=list(all_results.keys()), y=list(all_results.values()))
plt.xticks(rotation=45)
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.title("Comparison of ML and DL Models for Liver Disease Prediction")
plt.tight_layout()
plt.show()

def plot_history(history):
    fig, (ax1, ax2) = plt.subplots(1,2,figsize= (10,4))
    ax1.plot(history.history['loss'], label='loss')
    ax1.plot(history.history['val_loss'], label='val_loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Binary crossentropy')
    ax1.grid(True)

    ax2.plot(history.history['accuracy'], label='accuracy')
    ax2.plot(history.history['val_accuracy'], label='val_accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.grid(True)
    plt.show()

def train_model(X_train, y_train, num_nodes, dropoutt_prob,lr, batch_size, epochs):
  nn_model = tf.keras.Sequential([
      tf.keras.layers.Dense(num_nodes, activation='relu',input_shape = (10,)),
      tf.keras.layers.Dropout(dropoutt_prob),
      tf.keras.layers.Dense(num_nodes, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])

  nn_model.compile(optimizer = tf.keras.optimizers.Adam(lr), loss='binary_crossentropy',
                  metrics=['accuracy'])
  history = nn_model.fit(
    X_train, y_train,
    epochs=epochs,batch_size = batch_size, validation_split=0.2,verbose=0
  )
  return nn_model,history

least_val_loss = float('inf')
least_loss_model = None
epochs = 2

for num_nodes in [16, 32, 64]:
    for dropout_prob in [0, 0.2]:
        for lr in [0.1, 0.005, 0.001]:
            for batch_size in [32, 64, 128]:
                print(f"{num_nodes} nodes, dropout {dropout_prob}, lr {lr}, batch size {batch_size}")
                model, history = train_model(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs)
                plot_history(history)
                val_loss = model.evaluate(X_valid, y_valid)
                val_loss = val_loss[0] if isinstance(val_loss, list) else val_loss
                if val_loss < least_val_loss:
                    least_val_loss = val_loss
                    least_loss_model = model

y_pred = least_loss_model.predict(X_test)
y_pred = (y_pred > 0.5 ).astype(int).reshape(-1,)

# @title

print(classification_report(y_test, y_pred))

