# -*- coding: utf-8 -*-
"""pracice 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BAYviQkbRysph3i4TZ3bZjmCHyAs8qyn
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing  import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.impute import SimpleImputer

def is_line_valid(line):
    # A simple heuristic: check if quotes are balanced
    return line.count('"') % 2 == 0

input_file = "job_descriptions.csv"
output_file = "job_descriptions_cleaned.csv"
max_lines = 10000

written_lines = 0

with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:
    for line in infile:
        if is_line_valid(line):
            outfile.write(line)
            written_lines += 1
        if written_lines >= max_lines:
            break

print(f"Cleaning done. {written_lines} valid lines written to {output_file}.")

import pandas as pd
df = pd.read_csv("job_descriptions_cleaned.csv")
selected_columns = ['Job Id', 'Experience',"Qualifications",'Company','Preference', 'Salary Range', 'Country', 'Work Type', 'Job Title']
df = df[selected_columns]
print(df.head())

cols = df.columns
df.head()

# imputer= SimpleImputer(strategy='most_frequent')
# imputer.fit(df.iloc[:, 0:3])
# df.iloc[:, 0:3] = imputer.transform(df.iloc[:, 0:3])
# # df.head()

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='most_frequent')

# Select all columns except the first one (index 0)
imputer.fit(df.iloc[:, 1:])
df.iloc[:, 1:] = imputer.transform(df.iloc[:, 1:])

imputer= SimpleImputer(strategy='mean')
imputer.fit(df.iloc[:, 0:1])
df.iloc[:, 0:1] = imputer.transform(df.iloc[:, 0:1])
df.head()

df["Work Type"] = (df["Work Type"] == 'Intern').astype(int)

# Move "Work Type" to the end
cols = [col for col in df.columns if col != "Work Type"] + ["Work Type"]
df = df[cols]


df.head()

# import matplotlib.pyplot as plt

# for label in cols[:-1]:  # Assuming cols is your list of columns

#         # Plot histogram for Experience_Years > 20 (blue for 'gamma')
#         plt.hist(df[df["Work Type"] == 'Intern'][label],
#                  color='blue', label='Intern', alpha=0.7, density=True)

#         # Plot histogram for Experience_Years <= 20 (red for 'hadron')
#         plt.hist(df[df["Work Type"] != 'Intern'][label],
#                  color='red', label='!Intern', alpha=0.7, density=True)

#         # Set the title, labels, and legend
#         plt.title(f"Distribution of {label}")
#         plt.ylabel("Probability")
#         plt.xlabel(label)
#         plt.legend()  # Call legend after both histograms are plotted

#         # Show the plot
#         plt.show()

#Catgorical data
from sklearn.preprocessing import LabelEncoder
label_encoder_x= LabelEncoder()
df.iloc[:, 1]= label_encoder_x.fit_transform(df.iloc[:, 1])
df.iloc[:, 2]= label_encoder_x.fit_transform(df.iloc[:, 2])
df.iloc[:, 3]= label_encoder_x.fit_transform(df.iloc[:, 3])
df.iloc[:, 4]= label_encoder_x.fit_transform(df.iloc[:, 4])
df.iloc[:, 5]= label_encoder_x.fit_transform(df.iloc[:, 5])
df.iloc[:, 6]= label_encoder_x.fit_transform(df.iloc[:, 6])
df.iloc[:, 7]= label_encoder_x.fit_transform(df.iloc[:, 7])
# df.iloc[:, 8]= label_encoder_x.fit_transform(df.iloc[:, 8])
df.head()

import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# Define categorical columns
categorical_columns = ["Experience","Qualifications","Company","Preference", "Salary Range", "Country", "Job Title"]

# Create ColumnTransformer
ct = ColumnTransformer(
    transformers=[("onehot", OneHotEncoder(sparse_output=False), categorical_columns)],
    remainder='passthrough'
)

# Fit and transform the data
df_encoded = ct.fit_transform(df)

# Get new column names from encoder
onehot_feature_names = ct.named_transformers_["onehot"].get_feature_names_out(categorical_columns)
other_columns = [col for col in df.columns if col not in categorical_columns]
final_columns = list(onehot_feature_names) + other_columns

# Create final DataFrame
df = pd.DataFrame(df_encoded, columns=final_columns)

# Show result
print(df.head())

train, valid , test = np.split(df.sample(frac = 1), [int(0.6 * len(df)) , int(0.8 * len(df))])
df.head()

def scale_dataset(dataframe, oversample = False):
  x = dataframe[dataframe.columns[:-1]].values
  y = dataframe[dataframe.columns[-1]].values

  scaler = StandardScaler()
  x = scaler.fit_transform(x)
  if oversample:
    ros = RandomOverSampler()
    x,y =ros.fit_resample(x,y)
  data = np.hstack((x,np.reshape(y,(-1,1))))
  return data,x,y

train, X_train, y_train = scale_dataset(train, oversample = False)
valid, X_valid, y_valid = scale_dataset(valid, oversample = False)
test, X_test, y_test = scale_dataset(test, oversample = False)

# y_train = y_train.astype(int)
# y_test = y_test.astype(int)
# y_valid = y_valid.astype(int)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
knn_model = KNeighborsClassifier(n_neighbors = 3)
knn_model.fit(X_train,y_train)

y_pred = knn_model.predict(X_test)

print(classification_report(y_test, y_pred))

from sklearn.svm import SVC
svm_model = SVC()
svm_model = svm_model.fit(X_train,y_train)
y_pred = svm_model.predict(X_test)
print(classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression()
lr_model = lr_model.fit(X_train,y_train)

y_pred = lr_model.predict(X_test)
print(classification_report(y_test, y_pred))

"""Neural Network"""

import tensorflow as  tf

def plot_history(history):
    fig, (ax1, ax2) = plt.subplots(1,2,figsize= (10,4))
    ax1.plot(history.history['loss'], label='loss')
    ax1.plot(history.history['val_loss'], label='val_loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Binary crossentropy')
    ax1.grid(True)

    ax2.plot(history.history['accuracy'], label='accuracy')
    ax2.plot(history.history['val_accuracy'], label='val_accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.grid(True)
    plt.show()

import tensorflow as tf

def train_model(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs):
    input_shape = (X_train.shape[1],)  # Get the number of features dynamically


    nn_model = tf.keras.Sequential([
        tf.keras.layers.Dense(num_nodes, activation='relu', input_shape=input_shape),  # Use dynamic input shape
        tf.keras.layers.Dropout(dropout_prob),
        tf.keras.layers.Dense(num_nodes, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    # Compile the model
    nn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
                     loss='binary_crossentropy', metrics=['accuracy'])

    # Fit the model
    history = nn_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                           validation_split=0.2, verbose=0)

    return nn_model, history

least_val_loss = float('inf')
least_loss_model = None
epochs = 5

for num_nodes in [16, 32, 64]:
    for dropout_prob in [0, 0.2]:
        for lr in [0.1, 0.005, 0.001]:
            for batch_size in [32, 64, 128]:
                print(f"{num_nodes} nodes, dropout {dropout_prob}, lr {lr}, batch size {batch_size}")
                model, history = train_model(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs)
                plot_history(history)
                val_loss = model.evaluate(X_valid, y_valid)
                val_loss = val_loss[0] if isinstance(val_loss, list) else val_loss
                if val_loss < least_val_loss:
                    least_val_loss = val_loss
                    least_loss_model = model

y_pred = least_loss_model.predict(X_test)
y_pred = (y_pred > 0.5 ).astype(int).reshape(-1,)

print(classification_report(y_test, y_pred))

